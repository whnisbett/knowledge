"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[745],{6689:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var s=r(5893),i=r(1151);const t={title:"Hugging Face NLP Course"},a="Hugging Face NLP Course",l={id:"machine-learning/hugging-face-nlp-course",title:"Hugging Face NLP Course",description:"- My notes from Hugging Face's NPL Course",source:"@site/docs/machine-learning/hugging-face-nlp-course.md",sourceDirName:"machine-learning",slug:"/machine-learning/hugging-face-nlp-course",permalink:"/knowledge/machine-learning/hugging-face-nlp-course",draft:!1,unlisted:!1,editUrl:"https://github.com/whnisbett/knowledge/tree/main/docs/machine-learning/hugging-face-nlp-course.md",tags:[],version:"current",frontMatter:{title:"Hugging Face NLP Course"},sidebar:"tutorialSidebar",previous:{title:"Machine Learning",permalink:"/knowledge/machine-learning/"},next:{title:"Mathematics",permalink:"/knowledge/mathematics/"}},o={},c=[{value:"Chapter 1 \u2013 Transformer Models",id:"chapter-1--transformer-models",level:2},{value:"<code>pipeline</code> models",id:"pipeline-models",level:3},{value:"Transformer History",id:"transformer-history",level:3},{value:"Transformer Architecture and Training",id:"transformer-architecture-and-training",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"hugging-face-nlp-course",children:"Hugging Face NLP Course"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["My notes from ",(0,s.jsx)(n.a,{href:"https://huggingface.co/learn/nlp-course/en/chapter1/1",children:"Hugging Face's NPL Course"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chapter-1--transformer-models",children:"Chapter 1 \u2013 Transformer Models"}),"\n",(0,s.jsxs)(n.h3,{id:"pipeline-models",children:[(0,s.jsx)(n.code,{children:"pipeline"})," models"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Common NLP tasks","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Classifying whole sentences (e.g. sentiment analysis, spam detection)"}),"\n",(0,s.jsx)(n.li,{children:"Classifying each word in a sentence (e.g. NER, PoS tagging)"}),"\n",(0,s.jsx)(n.li,{children:"Generating text content"}),"\n",(0,s.jsx)(n.li,{children:"Extracting an answer from a text"}),"\n",(0,s.jsx)(n.li,{children:"Generating a new sentence from an input text"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"transformers"})," package offers a number of E2E models through ",(0,s.jsx)(n.code,{children:"pipeline"})," interface \u2013 all you need to do is specify the task"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from transformers import pipeline\n\nsent_classifier = pipeline(task="sentiment-analysis")\nsent_classifier("I didn\'t love this restaurant")\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.task",children:"Currently accepted tasks"})," that you can pass to pipeline"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"pipeline"})," takes care of pre-processing, inference, and post-processing"]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/models",children:"Index of all Hugging Face models"})}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"transformer-history",children:"Transformer History"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["June 2017: Transformer model is introduced by Google in ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03762",children:"Attention Is All You Need"})]}),"\n",(0,s.jsxs)(n.li,{children:["June 2018: The first pretrained Transformer model, GPT (Generative Pretrained Transformer), is introduced by OpenAI in ",(0,s.jsx)(n.a,{href:"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf",children:"Improving Language Understanding by Generative Pre-Training"})]}),"\n",(0,s.jsxs)(n.li,{children:["October 2018: BERT (Bidirectional Encoder Representations from Transformers) is introduced by Google in ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1810.04805",children:"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"})]}),"\n",(0,s.jsxs)(n.li,{children:["February 2019: GPT-2 is introduced by OpenAI in ",(0,s.jsx)(n.a,{href:"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",children:"Language Models are Unsupervised Multitask Learners"})]}),"\n",(0,s.jsxs)(n.li,{children:["October 2019: DistilBERT introduced by Google in ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1910.01108",children:"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"})]}),"\n",(0,s.jsxs)(n.li,{children:["October 2019: Facebook AI introduces ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/1910.13461.pdf",children:"BART"})," and Google introduces ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/1910.10683.pdf",children:"T5"})]}),"\n",(0,s.jsxs)(n.li,{children:["May 2020: GPT-3 introduced by OpenAI in ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2005.14165",children:"Language Models are Few-Shot Learners"}),"\u2013 the model performs well on a variety of tasks without any fine-tuning (called ",(0,s.jsx)(n.em,{children:"zero-shot learning"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:["These models can be broadly ",(0,s.jsx)(n.strong,{children:"grouped into 3 categories"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPT-like: auto-regressive transformers"}),"\n",(0,s.jsx)(n.li,{children:"BERT-like: auto-encoding transformers"}),"\n",(0,s.jsx)(n.li,{children:"BART/T5-like: seq2seq transformers"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"transformer-architecture-and-training",children:"Transformer Architecture and Training"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Leveraging transfer learning in the context of transformers is referred to as ",(0,s.jsx)(n.em,{children:"fine-tuning"})," a pretrained model"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"resources",children:"Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/huggingface/notebooks",children:"Hugging Face Notebooks"})}),"\n",(0,s.jsx)(n.li,{}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},1151:(e,n,r)=>{r.d(n,{Z:()=>l,a:()=>a});var s=r(7294);const i={},t=s.createContext(i);function a(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);